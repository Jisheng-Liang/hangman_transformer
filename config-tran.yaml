dataset: 'dataset/' # dataset directory
models: 'models/' # for storing models
plots: 'plots/' # for plots
pickle: 'pickle/' # pickle dumps root path

cuda: True #whether to use NVIDIA cuda
    
test_per_epoch: 0 #test per epoch i.e. how many times in ONE epoch
test_every_epoch: 5 #after how many epochs
print_per_epoch: 1 #print loss function how often in each epoch
save_every: 400 #save models after how many epochs
plot_every: 25 #save plots for test loss/train loss/accuracy

resume: True #resume training from saved model

lr: 0.0005 #learning rate

drop_uniform: False #whether dropping of character sets is independent of set size
reset_after: 400 #generate a new random dataset after these manh epochs
vocab_size: 26 #size of vocabulary. 26 engliush letters in our case
min_len: 2 #words with length less than min_len are not added to the dataset

model_name: 'Transformers' #type of RNN. Can be LSTM/GRU
use_embedding: False #whether to use character embeddings
embedding_dim: 512 #if use_embedding, dimension of embedding
hidden_dim: 2048 #hidden dimension of Transformer
num_head: 8
output_mid_features: 256 #number of neurons in hidden layer after RNN
miss_linear_dim: 256 #miss chars are projected to this dimension using a simple linear layer
num_layers: 2 #number of layers in RNN
dropout: 0.3 #dropout
batch_size: 2000 #batch size for training and testing
epochs: 3000 #total no. of epochs to train